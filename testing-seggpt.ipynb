{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'Painter' already exists and is not an empty directory.\n",
      "/home/andrewhealey/autodistill-seggpt/Painter/SegGPT/SegGPT_inference\n",
      "--2023-07-17 21:59:01--  https://huggingface.co/BAAI/SegGPT/resolve/main/seggpt_vit_large.pth\n",
      "Resolving huggingface.co (huggingface.co)... 99.84.160.57, 99.84.160.64, 99.84.160.43, ...\n",
      "Connecting to huggingface.co (huggingface.co)|99.84.160.57|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://cdn-lfs.huggingface.co/repos/a0/5c/a05ca640d6e4f08b928489d7e7f80507590b071210c55fc80e463f64c139936c/59c74d1993ca50ac0ae968d9bbe5db453f50aa5d5c7c9564cce6011c18b16570?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27seggpt_vit_large.pth%3B+filename%3D%22seggpt_vit_large.pth%22%3B&Expires=1689890342&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTY4OTg5MDM0Mn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy9hMC81Yy9hMDVjYTY0MGQ2ZTRmMDhiOTI4NDg5ZDdlN2Y4MDUwNzU5MGIwNzEyMTBjNTVmYzgwZTQ2M2Y2NGMxMzk5MzZjLzU5Yzc0ZDE5OTNjYTUwYWMwYWU5NjhkOWJiZTVkYjQ1M2Y1MGFhNWQ1YzdjOTU2NGNjZTYwMTFjMThiMTY1NzA%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=g50j38UqVhO3h5ubhAzQuAOSg5LvkdccGEUWiMNhPQbUaCmmc6UFrziAT5e0iOgPmFASjYOxxmGybESIbRXdywaG3NHrKIvz5KACdhNWcarID80JVd2V88r7ykbHmzfNPj-zpQybnfaKkPDprIzLl81oQ6MXuRdELY-rGJ0S3yfniqem8UkpHh-hGU57KcTrx7GI5h2sj%7EB6Fs%7EJMKfGQuiKEUYHPXl9HJinTyA8I%7EtrRUDK9goDSnWWevkEG4dWMcEYCbkybLaZD9Oms2ow3pW9Uk3G-u9p93EtRxuFM3%7EAJaWkTzv61emFTnxEHOKAJ-raPz2vo3MPVFx9hfK%7EBw__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
      "--2023-07-17 21:59:02--  https://cdn-lfs.huggingface.co/repos/a0/5c/a05ca640d6e4f08b928489d7e7f80507590b071210c55fc80e463f64c139936c/59c74d1993ca50ac0ae968d9bbe5db453f50aa5d5c7c9564cce6011c18b16570?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27seggpt_vit_large.pth%3B+filename%3D%22seggpt_vit_large.pth%22%3B&Expires=1689890342&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTY4OTg5MDM0Mn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy9hMC81Yy9hMDVjYTY0MGQ2ZTRmMDhiOTI4NDg5ZDdlN2Y4MDUwNzU5MGIwNzEyMTBjNTVmYzgwZTQ2M2Y2NGMxMzk5MzZjLzU5Yzc0ZDE5OTNjYTUwYWMwYWU5NjhkOWJiZTVkYjQ1M2Y1MGFhNWQ1YzdjOTU2NGNjZTYwMTFjMThiMTY1NzA%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=g50j38UqVhO3h5ubhAzQuAOSg5LvkdccGEUWiMNhPQbUaCmmc6UFrziAT5e0iOgPmFASjYOxxmGybESIbRXdywaG3NHrKIvz5KACdhNWcarID80JVd2V88r7ykbHmzfNPj-zpQybnfaKkPDprIzLl81oQ6MXuRdELY-rGJ0S3yfniqem8UkpHh-hGU57KcTrx7GI5h2sj%7EB6Fs%7EJMKfGQuiKEUYHPXl9HJinTyA8I%7EtrRUDK9goDSnWWevkEG4dWMcEYCbkybLaZD9Oms2ow3pW9Uk3G-u9p93EtRxuFM3%7EAJaWkTzv61emFTnxEHOKAJ-raPz2vo3MPVFx9hfK%7EBw__&Key-Pair-Id=KVTP0A1DKRTAX\n",
      "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 13.249.85.12, 13.249.85.23, 13.249.85.116, ...\n",
      "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|13.249.85.12|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1483023073 (1.4G) [binary/octet-stream]\n",
      "Saving to: ‘seggpt_vit_large.pth.1’\n",
      "\n",
      "seggpt_vit_large.pt 100%[===================>]   1.38G  84.6MB/s    in 17s     \n",
      "\n",
      "2023-07-17 21:59:18 (84.0 MB/s) - ‘seggpt_vit_large.pth.1’ saved [1483023073/1483023073]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/baaivision/Painter\n",
    "%cd Painter/SegGPT/SegGPT_inference\n",
    "!wget https://huggingface.co/BAAI/SegGPT/resolve/main/seggpt_vit_large.pth\n",
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in ./.venv/lib/python3.7/site-packages (1.21.6)\n",
      "Requirement already satisfied: opencv-python in ./.venv/lib/python3.7/site-packages (4.8.0.74)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting torch\n",
      "  Using cached torch-1.13.1-cp37-cp37m-manylinux1_x86_64.whl (887.5 MB)\n",
      "Collecting typing-extensions (from torch)\n",
      "  Obtaining dependency information for typing-extensions from https://files.pythonhosted.org/packages/ec/6b/63cc3df74987c36fe26157ee12e09e8f9db4de771e0f3404263117e75b95/typing_extensions-4.7.1-py3-none-any.whl.metadata\n",
      "  Downloading typing_extensions-4.7.1-py3-none-any.whl.metadata (3.1 kB)\n",
      "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch)\n",
      "  Using cached nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
      "Collecting nvidia-cudnn-cu11==8.5.0.96 (from torch)\n",
      "  Using cached nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
      "Collecting nvidia-cublas-cu11==11.10.3.66 (from torch)\n",
      "  Using cached nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
      "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch)\n",
      "  Using cached nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (68.0.0)\n",
      "Collecting wheel (from nvidia-cublas-cu11==11.10.3.66->torch)\n",
      "  Using cached wheel-0.40.0-py3-none-any.whl (64 kB)\n",
      "Using cached typing_extensions-4.7.1-py3-none-any.whl (33 kB)\n",
      "Installing collected packages: wheel, typing-extensions, nvidia-cuda-nvrtc-cu11, nvidia-cuda-runtime-cu11, nvidia-cublas-cu11, nvidia-cudnn-cu11, torch\n",
      "Successfully installed nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 torch-1.13.1 typing-extensions-4.7.1 wheel-0.40.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Found existing installation: nvidia-cublas-cu11 11.10.3.66\n",
      "Uninstalling nvidia-cublas-cu11-11.10.3.66:\n",
      "  Successfully uninstalled nvidia-cublas-cu11-11.10.3.66\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install numpy opencv-python\n",
    "%pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/facebookresearch/detectron2.git\n",
      "  Cloning https://github.com/facebookresearch/detectron2.git to /var/tmp/pip-req-build-24swtkqe\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/detectron2.git /var/tmp/pip-req-build-24swtkqe\n",
      "  Resolved https://github.com/facebookresearch/detectron2.git to commit 47d8ed7a1bd999719825f2e8a11d5b2828bc02c4\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting timm\n",
      "  Obtaining dependency information for timm from https://files.pythonhosted.org/packages/29/90/94f5deb8d76e24a89813aef95e8809ca8fd7414490428480eda19b133d4a/timm-0.9.2-py3-none-any.whl.metadata\n",
      "  Downloading timm-0.9.2-py3-none-any.whl.metadata (68 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.5/68.5 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting fairscale\n",
      "  Using cached fairscale-0.4.6.tar.gz (248 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.7 in ./.venv/lib/python3.7/site-packages (from timm) (1.13.1)\n",
      "Collecting torchvision (from timm)\n",
      "  Using cached torchvision-0.14.1-cp37-cp37m-manylinux1_x86_64.whl (24.2 MB)\n",
      "Collecting pyyaml (from timm)\n",
      "  Using cached PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
      "Collecting huggingface-hub (from timm)\n",
      "  Obtaining dependency information for huggingface-hub from https://files.pythonhosted.org/packages/7f/c4/adcbe9a696c135578cabcbdd7331332daad4d49b7c43688bc2d36b3a47d2/huggingface_hub-0.16.4-py3-none-any.whl.metadata\n",
      "  Downloading huggingface_hub-0.16.4-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting safetensors (from timm)\n",
      "  Downloading safetensors-0.3.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting Pillow>=7.1 (from detectron2==0.6)\n",
      "  Downloading Pillow-9.5.0-cp37-cp37m-manylinux_2_28_x86_64.whl (3.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m91.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting matplotlib (from detectron2==0.6)\n",
      "  Using cached matplotlib-3.5.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.2 MB)\n",
      "Collecting pycocotools>=2.0.2 (from detectron2==0.6)\n",
      "  Using cached pycocotools-2.0.6.tar.gz (24 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting termcolor>=1.1 (from detectron2==0.6)\n",
      "  Using cached termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
      "Collecting yacs>=0.1.8 (from detectron2==0.6)\n",
      "  Using cached yacs-0.1.8-py3-none-any.whl (14 kB)\n",
      "Collecting tabulate (from detectron2==0.6)\n",
      "  Using cached tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Collecting cloudpickle (from detectron2==0.6)\n",
      "  Using cached cloudpickle-2.2.1-py3-none-any.whl (25 kB)\n",
      "Collecting tqdm>4.29.0 (from detectron2==0.6)\n",
      "  Using cached tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
      "Collecting tensorboard (from detectron2==0.6)\n",
      "  Using cached tensorboard-2.11.2-py3-none-any.whl (6.0 MB)\n",
      "Collecting fvcore<0.1.6,>=0.1.5 (from detectron2==0.6)\n",
      "  Using cached fvcore-0.1.5.post20221221.tar.gz (50 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting iopath<0.1.10,>=0.1.7 (from detectron2==0.6)\n",
      "  Using cached iopath-0.1.9-py3-none-any.whl (27 kB)\n",
      "Collecting omegaconf>=2.1 (from detectron2==0.6)\n",
      "  Using cached omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
      "Collecting hydra-core>=1.1 (from detectron2==0.6)\n",
      "  Using cached hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
      "Collecting black (from detectron2==0.6)\n",
      "  Downloading black-23.3.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m88.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in ./.venv/lib/python3.7/site-packages (from detectron2==0.6) (23.1)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.7/site-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6) (1.21.6)\n",
      "Collecting antlr4-python3-runtime==4.9.* (from hydra-core>=1.1->detectron2==0.6)\n",
      "  Using cached antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting importlib-resources (from hydra-core>=1.1->detectron2==0.6)\n",
      "  Using cached importlib_resources-5.12.0-py3-none-any.whl (36 kB)\n",
      "Collecting portalocker (from iopath<0.1.10,>=0.1.7->detectron2==0.6)\n",
      "  Using cached portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib->detectron2==0.6)\n",
      "  Using cached cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib->detectron2==0.6)\n",
      "  Using cached fonttools-4.38.0-py3-none-any.whl (965 kB)\n",
      "Collecting kiwisolver>=1.0.1 (from matplotlib->detectron2==0.6)\n",
      "  Using cached kiwisolver-1.4.4-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.1 MB)\n",
      "Collecting pyparsing>=2.2.1 (from matplotlib->detectron2==0.6)\n",
      "  Obtaining dependency information for pyparsing>=2.2.1 from https://files.pythonhosted.org/packages/a4/24/6ae4c9c45cf99d96b06b5d99e25526c060303171fb0aea9da2bfd7dbde93/pyparsing-3.1.0-py3-none-any.whl.metadata\n",
      "  Downloading pyparsing-3.1.0-py3-none-any.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./.venv/lib/python3.7/site-packages (from matplotlib->detectron2==0.6) (2.8.2)\n",
      "Requirement already satisfied: typing-extensions in ./.venv/lib/python3.7/site-packages (from torch>=1.7->timm) (4.7.1)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in ./.venv/lib/python3.7/site-packages (from torch>=1.7->timm) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in ./.venv/lib/python3.7/site-packages (from torch>=1.7->timm) (8.5.0.96)\n",
      "Collecting nvidia-cublas-cu11==11.10.3.66 (from torch>=1.7->timm)\n",
      "  Using cached nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in ./.venv/lib/python3.7/site-packages (from torch>=1.7->timm) (11.7.99)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.7->timm) (68.0.0)\n",
      "Requirement already satisfied: wheel in ./.venv/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.7->timm) (0.40.0)\n",
      "Collecting click>=8.0.0 (from black->detectron2==0.6)\n",
      "  Obtaining dependency information for click>=8.0.0 from https://files.pythonhosted.org/packages/22/b3/1da4ea0efa2e5ae410a347be614162ca08bd24a84059938aa5122d1e751b/click-8.1.5-py3-none-any.whl.metadata\n",
      "  Downloading click-8.1.5-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting mypy-extensions>=0.4.3 (from black->detectron2==0.6)\n",
      "  Using cached mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Collecting pathspec>=0.9.0 (from black->detectron2==0.6)\n",
      "  Using cached pathspec-0.11.1-py3-none-any.whl (29 kB)\n",
      "Collecting platformdirs>=2 (from black->detectron2==0.6)\n",
      "  Obtaining dependency information for platformdirs>=2 from https://files.pythonhosted.org/packages/6d/a7/47b7088a28c8fe5775eb15281bf44d39facdbe4bc011a95ccb89390c2db9/platformdirs-3.9.1-py3-none-any.whl.metadata\n",
      "  Downloading platformdirs-3.9.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting tomli>=1.1.0 (from black->detectron2==0.6)\n",
      "  Using cached tomli-2.0.1-py3-none-any.whl (12 kB)\n",
      "Collecting typed-ast>=1.4.2 (from black->detectron2==0.6)\n",
      "  Obtaining dependency information for typed-ast>=1.4.2 from https://files.pythonhosted.org/packages/32/f1/75bd58fb1410cb72fbc6e8adf163015720db2c38844b46a9149c5ff6bf38/typed_ast-1.5.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading typed_ast-1.5.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting filelock (from huggingface-hub->timm)\n",
      "  Obtaining dependency information for filelock from https://files.pythonhosted.org/packages/00/45/ec3407adf6f6b5bf867a4462b2b0af27597a26bd3cd6e2534cb6ab029938/filelock-3.12.2-py3-none-any.whl.metadata\n",
      "  Downloading filelock-3.12.2-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting fsspec (from huggingface-hub->timm)\n",
      "  Using cached fsspec-2023.1.0-py3-none-any.whl (143 kB)\n",
      "Collecting requests (from huggingface-hub->timm)\n",
      "  Obtaining dependency information for requests from https://files.pythonhosted.org/packages/70/8e/0e2d847013cb52cd35b38c009bb167a1a26b2ce6cd6965bf26b47bc0bf44/requests-2.31.0-py3-none-any.whl.metadata\n",
      "  Downloading requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting importlib-metadata (from huggingface-hub->timm)\n",
      "  Obtaining dependency information for importlib-metadata from https://files.pythonhosted.org/packages/ff/94/64287b38c7de4c90683630338cf28f129decbba0a44f0c6db35a873c73c4/importlib_metadata-6.7.0-py3-none-any.whl.metadata\n",
      "  Downloading importlib_metadata-6.7.0-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting absl-py>=0.4 (from tensorboard->detectron2==0.6)\n",
      "  Using cached absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "Collecting grpcio>=1.24.3 (from tensorboard->detectron2==0.6)\n",
      "  Obtaining dependency information for grpcio>=1.24.3 from https://files.pythonhosted.org/packages/54/83/d6957f030e9c95ae063741afa93992b974cde21646a00451b16b5473413d/grpcio-1.56.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading grpcio-1.56.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
      "Collecting google-auth<3,>=1.6.3 (from tensorboard->detectron2==0.6)\n",
      "  Obtaining dependency information for google-auth<3,>=1.6.3 from https://files.pythonhosted.org/packages/9c/8d/bff87fc722553a5691d8514da5523c23547f3894189ba03b57592e37bdc2/google_auth-2.22.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading google_auth-2.22.0-py2.py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard->detectron2==0.6)\n",
      "  Using cached google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard->detectron2==0.6)\n",
      "  Using cached Markdown-3.4.3-py3-none-any.whl (93 kB)\n",
      "Collecting protobuf<4,>=3.9.2 (from tensorboard->detectron2==0.6)\n",
      "  Using cached protobuf-3.20.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard->detectron2==0.6)\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m118.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0 (from tensorboard->detectron2==0.6)\n",
      "  Using cached tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard->detectron2==0.6)\n",
      "  Using cached Werkzeug-2.2.3-py3-none-any.whl (233 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.6)\n",
      "  Obtaining dependency information for cachetools<6.0,>=2.0.0 from https://files.pythonhosted.org/packages/a9/c9/c8a7710f2cedcb1db9224fdd4d8307c9e48cbddc46c18b515fefc0f1abbe/cachetools-5.3.1-py3-none-any.whl.metadata\n",
      "  Downloading cachetools-5.3.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.6)\n",
      "  Using cached pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.6)\n",
      "  Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: six>=1.9.0 in ./.venv/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.6) (1.16.0)\n",
      "Collecting urllib3<2.0 (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.6)\n",
      "  Obtaining dependency information for urllib3<2.0 from https://files.pythonhosted.org/packages/c5/05/c214b32d21c0b465506f95c4f28ccbcba15022e000b043b72b3df7728471/urllib3-1.26.16-py2.py3-none-any.whl.metadata\n",
      "  Downloading urllib3-1.26.16-py2.py3-none-any.whl.metadata (48 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.4/48.4 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->detectron2==0.6)\n",
      "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Collecting zipp>=0.5 (from importlib-metadata->huggingface-hub->timm)\n",
      "  Using cached zipp-3.15.0-py3-none-any.whl (6.8 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests->huggingface-hub->timm)\n",
      "  Obtaining dependency information for charset-normalizer<4,>=2 from https://files.pythonhosted.org/packages/89/f5/88e9dd454756fea555198ddbe6fa40d6408ec4f10ad4f0a911e0b7e471e4/charset_normalizer-3.2.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading charset_normalizer-3.2.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->huggingface-hub->timm)\n",
      "  Downloading idna-3.4-py3-none-any.whl (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.5/61.5 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting certifi>=2017.4.17 (from requests->huggingface-hub->timm)\n",
      "  Downloading certifi-2023.5.7-py3-none-any.whl (156 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m157.0/157.0 kB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting MarkupSafe>=2.1.1 (from werkzeug>=1.0.1->tensorboard->detectron2==0.6)\n",
      "  Obtaining dependency information for MarkupSafe>=2.1.1 from https://files.pythonhosted.org/packages/e5/dd/49576e803c0d974671e44fa78049217fcc68af3662a24f831525ed30e6c7/MarkupSafe-2.1.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading MarkupSafe-2.1.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting pyasn1<0.6.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->detectron2==0.6)\n",
      "  Using cached pyasn1-0.5.0-py2.py3-none-any.whl (83 kB)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->detectron2==0.6)\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Using cached timm-0.9.2-py3-none-any.whl (2.2 MB)\n",
      "Using cached huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
      "Using cached click-8.1.5-py3-none-any.whl (98 kB)\n",
      "Using cached google_auth-2.22.0-py2.py3-none-any.whl (181 kB)\n",
      "Downloading grpcio-1.56.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m104.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached importlib_metadata-6.7.0-py3-none-any.whl (22 kB)\n",
      "Using cached platformdirs-3.9.1-py3-none-any.whl (16 kB)\n",
      "Using cached pyparsing-3.1.0-py3-none-any.whl (102 kB)\n",
      "Downloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.6/62.6 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typed_ast-1.5.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (778 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m778.3/778.3 kB\u001b[0m \u001b[31m73.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached filelock-3.12.2-py3-none-any.whl (10 kB)\n",
      "Using cached cachetools-5.3.1-py3-none-any.whl (9.3 kB)\n",
      "Downloading charset_normalizer-3.2.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (175 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.8/175.8 kB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading MarkupSafe-2.1.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
      "Downloading urllib3-1.26.16-py2.py3-none-any.whl (143 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.1/143.1 kB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: fairscale, detectron2, fvcore, antlr4-python3-runtime, pycocotools\n",
      "  Building wheel for fairscale (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fairscale: filename=fairscale-0.4.6-py3-none-any.whl size=307224 sha256=f32a1dc274bbe491b6b261c0780880500b1e2d6ebfcaa62a7c2cbdc131ef70ac\n",
      "  Stored in directory: /home/andrewhealey/.cache/pip/wheels/4e/4f/0b/94c29ea06dfad93260cb0377855f87b7b863312317a7f69fe7\n",
      "  Building wheel for detectron2 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for detectron2: filename=detectron2-0.6-cp37-cp37m-linux_x86_64.whl size=4639808 sha256=35c7e57779eb6c80961f5d7fe53bd973c38223addce5603dcca8de869eb0e988\n",
      "  Stored in directory: /var/tmp/pip-ephem-wheel-cache-ju_qti87/wheels/07/dc/32/0322cb484dbefab8b9366bfedbaff5060ac7d149d69c27ca5d\n",
      "  Building wheel for fvcore (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61405 sha256=37a17e892b0334845cc3b9b7124ae3ee746f3db1e208dcee2be7e574d95f28f0\n",
      "  Stored in directory: /home/andrewhealey/.cache/pip/wheels/af/cd/23/3fb62ec8606cb08cc18abb8d67bec255baf353623be889da1e\n",
      "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=ef18904247b34dfe1caf413c1aebec9c6f26c3b664c2f5afb9b5fb905111918d\n",
      "  Stored in directory: /home/andrewhealey/.cache/pip/wheels/8b/8d/53/2af8772d9aec614e3fc65e53d4a993ad73c61daa8bbd85a873\n",
      "  Building wheel for pycocotools (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pycocotools: filename=pycocotools-2.0.6-cp37-cp37m-linux_x86_64.whl size=148657 sha256=38ce05a625b52861aa8ae943c32ae2b180ef680c7477cc282c10bc20c6fd4c69\n",
      "  Stored in directory: /home/andrewhealey/.cache/pip/wheels/06/f6/f9/9cc49c6de8e3cf27dfddd91bf46595a057141d4583a2adaf03\n",
      "Successfully built fairscale detectron2 fvcore antlr4-python3-runtime pycocotools\n",
      "Installing collected packages: tensorboard-plugin-wit, safetensors, antlr4-python3-runtime, zipp, urllib3, typed-ast, tqdm, tomli, termcolor, tensorboard-data-server, tabulate, pyyaml, pyparsing, pyasn1, protobuf, portalocker, platformdirs, Pillow, pathspec, oauthlib, nvidia-cublas-cu11, mypy-extensions, MarkupSafe, kiwisolver, idna, grpcio, fsspec, fonttools, filelock, cycler, cloudpickle, charset-normalizer, certifi, cachetools, absl-py, yacs, werkzeug, rsa, requests, pyasn1-modules, omegaconf, matplotlib, iopath, importlib-resources, importlib-metadata, requests-oauthlib, pycocotools, markdown, hydra-core, huggingface-hub, google-auth, fvcore, click, torchvision, google-auth-oauthlib, fairscale, black, timm, tensorboard, detectron2\n",
      "Successfully installed MarkupSafe-2.1.3 Pillow-9.5.0 absl-py-1.4.0 antlr4-python3-runtime-4.9.3 black-23.3.0 cachetools-5.3.1 certifi-2023.5.7 charset-normalizer-3.2.0 click-8.1.5 cloudpickle-2.2.1 cycler-0.11.0 detectron2-0.6 fairscale-0.4.6 filelock-3.12.2 fonttools-4.38.0 fsspec-2023.1.0 fvcore-0.1.5.post20221221 google-auth-2.22.0 google-auth-oauthlib-0.4.6 grpcio-1.56.0 huggingface-hub-0.16.4 hydra-core-1.3.2 idna-3.4 importlib-metadata-6.7.0 importlib-resources-5.12.0 iopath-0.1.9 kiwisolver-1.4.4 markdown-3.4.3 matplotlib-3.5.3 mypy-extensions-1.0.0 nvidia-cublas-cu11-11.10.3.66 oauthlib-3.2.2 omegaconf-2.3.0 pathspec-0.11.1 platformdirs-3.9.1 portalocker-2.7.0 protobuf-3.20.3 pyasn1-0.5.0 pyasn1-modules-0.3.0 pycocotools-2.0.6 pyparsing-3.1.0 pyyaml-6.0 requests-2.31.0 requests-oauthlib-1.3.1 rsa-4.9 safetensors-0.3.1 tabulate-0.9.0 tensorboard-2.11.2 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 termcolor-2.3.0 timm-0.9.2 tomli-2.0.1 torchvision-0.14.1 tqdm-4.65.0 typed-ast-1.5.5 urllib3-1.26.16 werkzeug-2.2.3 yacs-0.1.8 zipp-3.15.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install timm fairscale git+https://github.com/facebookresearch/detectron2.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: nvidia-cublas-cu11 11.10.3.66\n",
      "Uninstalling nvidia-cublas-cu11-11.10.3.66:\n",
      "  Successfully uninstalled nvidia-cublas-cu11-11.10.3.66\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip uninstall -y nvidia_cublas_cu11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/andrewhealey/autodistill-seggpt/Painter/SegGPT/SegGPT_inference\n",
      "/home/andrewhealey/autodistill-seggpt/Painter/SegGPT/SegGPT_inference\n"
     ]
    }
   ],
   "source": [
    "%cd Painter/SegGPT/SegGPT_inference/\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "!python3 seggpt_inference.py \\\n",
    "--input_image examples/hmbb_2.jpg \\\n",
    "--prompt_image examples/hmbb_1.jpg \\\n",
    "--prompt_target examples/hmbb_1_target.png \\\n",
    "--output_dir ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/andrewhealey/autodistill-seggpt/Painter/SegGPT/SegGPT_inference'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tumor Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n",
      "Downloading Dataset Version Zip in Cancer-Detection-4 to coco-segmentation: 100% [5324346 / 5324346] bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Dataset Version Zip to Cancer-Detection-4 in coco-segmentation:: 100%|██████████| 302/302 [00:00<00:00, 8026.71it/s]\n"
     ]
    }
   ],
   "source": [
    "%pip install -q roboflow pycocotools\n",
    "from roboflow import Roboflow\n",
    "rf = Roboflow(api_key=\"YsWaBWdLsFfUXb1x0YdP\")\n",
    "project = rf.workspace(\"amala02\").project(\"cancer-detection-xmvfg\")\n",
    "!rm -rf ./Cancer-Detection-4\n",
    "dataset = project.version(4).download(\"coco-segmentation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "from math import floor\n",
    "from random import random\n",
    "def random_rgb():\n",
    "  return np.asarray([floor(random()*256) for i in range(3)])\n",
    "random_rgb()\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pycocotools.coco import COCO\n",
    "data_dir = \"Cancer-Detection-4/train\"\n",
    "ref_limit = 1\n",
    "\n",
    "new_data_dir = \"new_data\"\n",
    "!rm -rf $new_data_dir\n",
    "!mkdir $new_data_dir\n",
    "\n",
    "# We designate our *own* \"train\"/\"test\" directories--there will be a few images in train/, and most images in test/.\n",
    "new_train_dir = os.path.join(new_data_dir,\"train\")\n",
    "new_test_dir = os.path.join(new_data_dir,\"test\")\n",
    "!mkdir $new_train_dir $new_test_dir\n",
    "\n",
    "ann_file = os.path.join(data_dir,\"_annotations.coco.json\")\n",
    "coco=COCO(ann_file)\n",
    "\n",
    "ref_ann_dir = os.path.join(new_train_dir,\"Annotations\")\n",
    "ref_img_dir = os.path.join(new_train_dir,\"Images\")\n",
    "test_ann_dir = os.path.join(new_test_dir,\"Annotations\")\n",
    "test_img_dir = os.path.join(new_test_dir,\"Images\")\n",
    "!mkdir $test_ann_dir $test_img_dir $ref_ann_dir $ref_img_dir\n",
    "\n",
    "cat_colors = {catId: random_rgb() for catId in coco.getCatIds()}\n",
    "\n",
    "num_train_imgs = 0\n",
    "for imgId in coco.getImgIds():\n",
    "\n",
    "  ann_dir = ref_ann_dir if num_train_imgs < ref_limit else test_ann_dir\n",
    "  img_dir = ref_img_dir if num_train_imgs < ref_limit else test_img_dir\n",
    "\n",
    "  img = coco.imgs[imgId]\n",
    "  img_path = os.path.join(data_dir,img[\"file_name\"])\n",
    "  img_cv2 = cv2.imread(img_path)\n",
    "\n",
    "  str_imgId = str(imgId).zfill(3)\n",
    "  new_img_path = os.path.join(img_dir,f\"{str_imgId}.jpg\")\n",
    "\n",
    "  !cp $img_path $new_img_path\n",
    "\n",
    "  mask = None#np.zeros_like(img_cv2)\n",
    "  mask_path = os.path.join(ann_dir,f\"{str_imgId}.png\")\n",
    "  for annId in coco.getAnnIds(imgIds=[imgId]):\n",
    "    ann = coco.anns[annId]\n",
    "    catId = ann[\"category_id\"]\n",
    "\n",
    "    ann_mask = coco.annToMask(ann)[:,:,None]\n",
    "    ann_mask = np.repeat(ann_mask,3,axis=-1) * cat_colors[catId][None,None,:]\n",
    "    ann_mask = ann_mask.astype(\"uint8\")\n",
    "\n",
    "    if mask is None:\n",
    "      mask = ann_mask\n",
    "    else:\n",
    "      mask += ann_mask\n",
    "  \n",
    "  cv2.imwrite(mask_path,mask)\n",
    "  num_train_imgs += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n",
      "cp: cannot stat 'output//': No such file or directory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from seggpt_engine import inference_image, inference_video\n",
    "import models_seggpt\n",
    "\n",
    "\n",
    "imagenet_mean = np.array([0.485, 0.456, 0.406])\n",
    "imagenet_std = np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "\n",
    "\n",
    "def prepare_model(chkpt_dir, arch='seggpt_vit_large_patch16_input896x448', seg_type='instance'):\n",
    "    # build model\n",
    "    model = getattr(models_seggpt, arch)()\n",
    "    model.seg_type = seg_type\n",
    "    # load model\n",
    "    checkpoint = torch.load(chkpt_dir, map_location='cpu')\n",
    "    msg = model.load_state_dict(checkpoint['model'], strict=False)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # args = get_args_parser()\n",
    "\n",
    "    ckpt_path = 'seggpt_vit_large.pth'\n",
    "    device = torch.device(\"cuda\")\n",
    "    model = \"seggpt_vit_large_patch16_input896x448\"\n",
    "    seg_type = \"instance\"\n",
    "\n",
    "    device = torch.device(device)\n",
    "    model = prepare_model(ckpt_path, model, seg_type).to(device)\n",
    "    print('Model loaded.')\n",
    "\n",
    "    input_images = glob(\"new_data/test/Images/*\")\n",
    "\n",
    "    prompt_images = glob(\"new_data/train/Images/*\")\n",
    "    prompt_masks = glob(\"new_data/train/Annotations/*\")\n",
    "    output_dir = \"output/\"\n",
    "\n",
    "    !rm -rf old_output/\n",
    "    !cp -r $output_dir/ old_output/\n",
    "    !rm -rf $output_dir\n",
    "    !mkdir -p $output_dir\n",
    "\n",
    "    for input_image in tqdm(input_images):\n",
    "        img_name = os.path.basename(input_image)\n",
    "        out_path = os.path.join(output_dir, '.'.join(img_name.split('.')[:-1]) + '.png')\n",
    "        inference_image(model, device, input_image, prompt_images, prompt_masks, out_path)\n",
    "\n",
    "    print('Finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from common import *\n",
    "# from load import *\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "eps = 1e-10\n",
    "\n",
    "def semseg_iou(gt_mask_dir: str, output_dir: str):\n",
    "    running_intersection = 0\n",
    "    running_union = 0\n",
    "    running_iou = 0\n",
    "    num_imgs = 0\n",
    "\n",
    "    for filename in os.listdir(gt_mask_dir):\n",
    "        gt_mask_path = os.path.join(gt_mask_dir,filename)\n",
    "        gt_mask_name = \".\".join(filename.split(\".\")[:-1])\n",
    "        output_mask_path = os.path.join(output_dir, gt_mask_name + \".png\")\n",
    "        if os.path.exists(output_mask_path):\n",
    "            output_mask = cv2.imread(output_mask_path) > 0.5\n",
    "            gt_mask = cv2.imread(gt_mask_path) > 0.5\n",
    "\n",
    "            # output_mask = np.any(output_mask,axis=2)\n",
    "            # gt_mask = np.any(gt_mask,axis=2)\n",
    "\n",
    "            intersection = np.logical_and(output_mask, gt_mask).sum()\n",
    "            union = np.logical_or(output_mask, gt_mask).sum()\n",
    "\n",
    "\n",
    "            running_intersection += intersection\n",
    "            running_union += union\n",
    "\n",
    "            iou = intersection / (union + eps)\n",
    "            running_iou += iou\n",
    "            num_imgs += 1\n",
    "\n",
    "    iou = running_intersection / (running_union + eps)\n",
    "\n",
    "    return iou,running_iou/num_imgs\n",
    "\n",
    "\n",
    "import argparse\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # args = get_arguments()\n",
    "    output_dir = \"./output\"\n",
    "    gt_dir = \"./new_data/test/Annotations\"\n",
    "\n",
    "    running_iou = 0\n",
    "    running_iou_2 = 0\n",
    "    num_dirs = 0\n",
    "\n",
    "    iou,iou_2 = semseg_iou(gt_dir, output_dir)\n",
    "    g = os.path.basename(gt_dir)\n",
    "    print(f\"{g}: {round(iou,3)}/{round(iou_2,3)}\")\n",
    "    num_dirs += 1\n",
    "    running_iou += iou\n",
    "    running_iou_2 += iou_2\n",
    "\n",
    "    miou = running_iou / (num_dirs + eps)\n",
    "    miou_2 = running_iou_2 / (num_dirs + eps)\n",
    "    print(f\"mIoU: {round(miou,3)}/{round(miou_2,3)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
